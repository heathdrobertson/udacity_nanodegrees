{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Exercise\n",
    "In this exercise, we'll build a model that, as you'll see, dramatically overfits the training data. This will allow you to see what overfitting can \"look like\" in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we'll use gradient boosted trees. In order to implement this model, we'll use the XGBoost package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/toilethill/.local/lib/python3.6/site-packages (0.82)\n",
      "Requirement already satisfied: numpy in /home/toilethill/.local/lib/python3.6/site-packages (from xgboost) (1.16.2)\n",
      "Requirement already satisfied: scipy in /home/toilethill/.local/lib/python3.6/site-packages (from xgboost) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install --user xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows in a dataframe\n",
    "def nrow(df): \n",
    "    return(len(df.index))\n",
    "\n",
    "# number of columns in a dataframe\n",
    "def ncol(df): \n",
    "    return(len(df.columns))\n",
    "\n",
    "# flatten nested lists/arrays\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# combine multiple arrays into a single list\n",
    "def c(*args):\n",
    "    return(flatten([item for item in args]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we're going to try to predict the returns of the S&P 500 ETF. This may be a futile endeavor, since many experts consider the S&P 500 to be essentially unpredictable, but it will serve well for the purpose of this exercise. The following cell loads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SPYZ.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data file has four columns, `Date`, `Close`, `Volume` and `Return`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>146.8750</td>\n",
       "      <td>3172700</td>\n",
       "      <td>0.001598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>145.4375</td>\n",
       "      <td>8164300</td>\n",
       "      <td>-0.009787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>139.7500</td>\n",
       "      <td>8089800</td>\n",
       "      <td>-0.039106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>140.0000</td>\n",
       "      <td>12177900</td>\n",
       "      <td>0.001789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>137.7500</td>\n",
       "      <td>6227200</td>\n",
       "      <td>-0.016071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Close    Volume    Return\n",
       "0  1999-12-31  146.8750   3172700  0.001598\n",
       "1  2000-01-03  145.4375   8164300 -0.009787\n",
       "2  2000-01-04  139.7500   8089800 -0.039106\n",
       "3  2000-01-05  140.0000  12177900  0.001789\n",
       "4  2000-01-06  137.7500   6227200 -0.016071"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4780"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = nrow(df)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll form our predictors/features. In the cells below, we create four types of features. We also use a parameter, `K`, to set the number of each type of feature to build. With a `K` of 25, 100 features will be created. This should already seem like a lot of features, and alert you to the potential that the model will be overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = []\n",
    "\n",
    "# we'll create a new DataFrame to hold the data that we'll use to train the model\n",
    "# we'll create it from the `Return` column in the original DataFrame, but rename that column `y`\n",
    "model_df = pd.DataFrame(data = df['Return']).rename(columns = {\"Return\" : \"y\"})\n",
    "\n",
    "# IMPORTANT: this sets how many of each of the following four predictors to create\n",
    "K = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable        Type         Data/Info\n",
      "--------------------------------------\n",
      "K               int          25\n",
      "X_test          DataFrame    Empty DataFrame\\nColumns:<...>\\n[1586 rows x 0 columns]\n",
      "X_train         DataFrame    Empty DataFrame\\nColumns:<...>\\n[3169 rows x 0 columns]\n",
      "Y_test          Series       3170    0.005014\\n3171   <...>\\nName: y, dtype: float64\n",
      "Y_train         Series       26      0.013608\\n27     <...>\\nName: y, dtype: float64\n",
      "autopep8        module       <module 'autopep8' from '<...>te-packages/autopep8.py'>\n",
      "breakpoint      int          3170\n",
      "c               function     <function c at 0x7fe390126598>\n",
      "df              DataFrame                Date       Cl<...>\\n[4780 rows x 4 columns]\n",
      "dtrain          DMatrix      <xgboost.core.DMatrix object at 0x7fe357a06a20>\n",
      "flatten         function     <function <lambda> at 0x7fe35eef3e18>\n",
      "i               int          4779\n",
      "json            module       <module 'json' from '/usr<...>hon3.6/json/__init__.py'>\n",
      "math            module       <module 'math' from '/usr<...>36m-x86_64-linux-gnu.so'>\n",
      "model           DataFrame                 y\\n25   -0.0<...>\\n[4755 rows x 1 columns]\n",
      "model_df        DataFrame                 y\\n0     0.0<...>\\n[4780 rows x 1 columns]\n",
      "n               int          4780\n",
      "ncol            function     <function ncol at 0x7fe35eef3598>\n",
      "np              module       <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "nrow            function     <function nrow at 0x7fe39044e620>\n",
      "num_round       int          20\n",
      "os              module       <module 'os' from '/usr/l<...>cal/lib/python3.6/os.py'>\n",
      "param           dict         n=2\n",
      "pd              module       <module 'pandas' from '/h<...>ages/pandas/__init__.py'>\n",
      "plt             module       <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "predictors      list         n=0\n",
      "test_data       DataFrame                 y\\n3170  0.0<...>\\n[1586 rows x 1 columns]\n",
      "train_size      float        0.6666666666666666\n",
      "training_data   DataFrame                 y\\n26    0.0<...>\\n[3169 rows x 1 columns]\n",
      "vol_df          DataFrame            Return       vol\\<...>\\n[4780 rows x 2 columns]\n",
      "xgb             module       <module 'xgboost' from '/<...>ges/xgboost/__init__.py'>\n",
      "yapf_reformat   function     <function yapf_reformat at 0x7fe390523488>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you write the code to create the four types of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for L in range(1,K+1): \n",
    "    # this predictor is just the return L days ago, where L goes from 1 to K\n",
    "    # these predictors will be named `R1`, `R2`, etc.\n",
    "    pR = \"\".join([\"R\",str(L)]) \n",
    "    predictors.append(pR)\n",
    "    for i in range(K+1,n): \n",
    "        # TODO: fill in the code to assign the return from L days before to the ith row of this predictor in `model_df`\n",
    "        model_df.loc[i, pR] = df.loc[i-L,'Return']\n",
    "\n",
    "    # this predictor is the return L days ago, squared, where L goes from 1 to K\n",
    "    # these predictors will be named `Rsq1`, `Rsq2`, etc.\n",
    "    pR2 = \"\".join([\"Rsq\",str(L)])\n",
    "    predictors.append(pR2)\n",
    "    for i in range(K+1,n): \n",
    "        # TODO: fill in the code to assign the squared return from L days before to the ith row of this predictor \n",
    "        # in `model_df`\n",
    "        model_df.loc[i, pR2] = (df.loc[i-L,'Return']) ** 2\n",
    "\n",
    "    # this predictor is the log volume L days ago, where L goes from 1 to K\n",
    "    # these predictors will be named `V1`, `V2`, etc.\n",
    "    pV = \"\".join([\"V\",str(L)])\n",
    "    predictors.append(pV)\n",
    "    for i in range(K+1,n): \n",
    "        # TODO: fill in the code to assign the log of the volume from L days before to the ith row of this predictor \n",
    "        # in `model_df`\n",
    "        # Add 1 to the volume before taking the log\n",
    "        model_df.loc[i, pV] = math.log(1.0 + df.loc[i-L,'Volume'])\n",
    "\n",
    "    # this predictor is the product of the return and the log volume from L days ago, where L goes from 1 to K\n",
    "    # these predictors will be named `RV1`, `RV2`, etc.\n",
    "    pRV = \"\".join([\"RV\",str(L)])\n",
    "    predictors.append(pRV)\n",
    "    for i in range(K+1,n): \n",
    "        # TODO: fill in the code to assign the product of the return and the log volume from L days before to the\n",
    "        # ith row of this predictor in `model_df`\n",
    "        model_df.loc[i, pRV] = model_df.loc[i, pR] * model_df.loc[i, pV]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the predictors we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>R1</th>\n",
       "      <th>Rsq1</th>\n",
       "      <th>V1</th>\n",
       "      <th>RV1</th>\n",
       "      <th>R2</th>\n",
       "      <th>Rsq2</th>\n",
       "      <th>V2</th>\n",
       "      <th>RV2</th>\n",
       "      <th>R3</th>\n",
       "      <th>...</th>\n",
       "      <th>V23</th>\n",
       "      <th>RV23</th>\n",
       "      <th>R24</th>\n",
       "      <th>Rsq24</th>\n",
       "      <th>V24</th>\n",
       "      <th>RV24</th>\n",
       "      <th>R25</th>\n",
       "      <th>Rsq25</th>\n",
       "      <th>V25</th>\n",
       "      <th>RV25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.016304</td>\n",
       "      <td>-0.014726</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>15.892349</td>\n",
       "      <td>-0.234024</td>\n",
       "      <td>-0.007529</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>16.198698</td>\n",
       "      <td>-0.121956</td>\n",
       "      <td>-0.018688</td>\n",
       "      <td>...</td>\n",
       "      <td>15.959991</td>\n",
       "      <td>0.076664</td>\n",
       "      <td>-0.009302</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>15.695540</td>\n",
       "      <td>-0.145995</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>16.209371</td>\n",
       "      <td>0.428273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-0.017157</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>16.221058</td>\n",
       "      <td>0.264474</td>\n",
       "      <td>-0.014726</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>15.892349</td>\n",
       "      <td>-0.234024</td>\n",
       "      <td>-0.007529</td>\n",
       "      <td>...</td>\n",
       "      <td>16.372203</td>\n",
       "      <td>-0.177882</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>15.959991</td>\n",
       "      <td>0.076664</td>\n",
       "      <td>-0.009302</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>15.695540</td>\n",
       "      <td>-0.145995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.001133</td>\n",
       "      <td>-0.017157</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>15.929221</td>\n",
       "      <td>-0.273290</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>16.221058</td>\n",
       "      <td>0.264474</td>\n",
       "      <td>-0.014726</td>\n",
       "      <td>...</td>\n",
       "      <td>16.461827</td>\n",
       "      <td>0.683503</td>\n",
       "      <td>-0.010865</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>16.372203</td>\n",
       "      <td>-0.177882</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>15.959991</td>\n",
       "      <td>0.076664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.034194</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>15.387039</td>\n",
       "      <td>0.017437</td>\n",
       "      <td>-0.017157</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>15.929221</td>\n",
       "      <td>-0.273290</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>...</td>\n",
       "      <td>15.858172</td>\n",
       "      <td>-0.178954</td>\n",
       "      <td>0.041520</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>16.461827</td>\n",
       "      <td>0.683503</td>\n",
       "      <td>-0.010865</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>16.372203</td>\n",
       "      <td>-0.177882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.034194</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>15.494960</td>\n",
       "      <td>0.529838</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>15.387039</td>\n",
       "      <td>0.017437</td>\n",
       "      <td>-0.017157</td>\n",
       "      <td>...</td>\n",
       "      <td>16.562480</td>\n",
       "      <td>-0.054770</td>\n",
       "      <td>-0.011285</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>15.858172</td>\n",
       "      <td>-0.178954</td>\n",
       "      <td>0.041520</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>16.461827</td>\n",
       "      <td>0.683503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y        R1      Rsq1         V1       RV1        R2      Rsq2  \\\n",
       "100  0.016304 -0.014726  0.000217  15.892349 -0.234024 -0.007529  0.000057   \n",
       "101 -0.017157  0.016304  0.000266  16.221058  0.264474 -0.014726  0.000217   \n",
       "102  0.001133 -0.017157  0.000294  15.929221 -0.273290  0.016304  0.000266   \n",
       "103  0.034194  0.001133  0.000001  15.387039  0.017437 -0.017157  0.000294   \n",
       "104  0.000657  0.034194  0.001169  15.494960  0.529838  0.001133  0.000001   \n",
       "\n",
       "            V2       RV2        R3    ...           V23      RV23       R24  \\\n",
       "100  16.198698 -0.121956 -0.018688    ...     15.959991  0.076664 -0.009302   \n",
       "101  15.892349 -0.234024 -0.007529    ...     16.372203 -0.177882  0.004804   \n",
       "102  16.221058  0.264474 -0.014726    ...     16.461827  0.683503 -0.010865   \n",
       "103  15.929221 -0.273290  0.016304    ...     15.858172 -0.178954  0.041520   \n",
       "104  15.387039  0.017437 -0.017157    ...     16.562480 -0.054770 -0.011285   \n",
       "\n",
       "        Rsq24        V24      RV24       R25     Rsq25        V25      RV25  \n",
       "100  0.000087  15.695540 -0.145995  0.026421  0.000698  16.209371  0.428273  \n",
       "101  0.000023  15.959991  0.076664 -0.009302  0.000087  15.695540 -0.145995  \n",
       "102  0.000118  16.372203 -0.177882  0.004804  0.000023  15.959991  0.076664  \n",
       "103  0.001724  16.461827  0.683503 -0.010865  0.000118  16.372203 -0.177882  \n",
       "104  0.000127  15.858172 -0.178954  0.041520  0.001724  16.461827  0.683503  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.iloc[100:105,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a DataFrame that holds the recent volatility of the ETF's returns, as measured by the standard deviation of a sliding window of the past 20 days' returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_df = pd.DataFrame(data = df[['Return']])\n",
    "\n",
    "for i in range(K+1,n): \n",
    "    # TODO: create the code to assign the standard deviation of the return from the time period starting \n",
    "    # 20 days before day i, up to the day before day i, to the ith row of `vol_df`\n",
    "    vol_df.loc[i, 'vol'] = np.std(vol_df.loc[(i-20):(i-1),'Return'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Return</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.016304</td>\n",
       "      <td>0.013069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-0.017157</td>\n",
       "      <td>0.013615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.014007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.034194</td>\n",
       "      <td>0.014008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.015792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Return       vol\n",
       "100  0.016304  0.013069\n",
       "101 -0.017157  0.013615\n",
       "102  0.001133  0.014007\n",
       "103  0.034194  0.014008\n",
       "104  0.000657  0.015792"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_df.iloc[100:105,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we can start thinking about training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training, we'll use all the data except for the first K days, for which the predictors' values are NaNs\n",
    "model = model_df.iloc[K:n,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, first split the data into train and test sets, and then split off the targets from the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_size = 2.0/3.0\n",
    "breakpoint = round(nrow(model) * train_size)\n",
    "\n",
    "# TODO: fill in the code to split off the chunk of data up to the breakpoint as the training set, and\n",
    "# assign the rest as the test set.\n",
    "training_data = model.iloc[1:breakpoint,:]\n",
    "test_data = model.loc[breakpoint : nrow(model),]\n",
    "\n",
    "# TODO: Split training data and test data into targets (Y) and predictors (X), for the training set and the test set\n",
    "X_train = training_data.iloc[:,1:ncol(training_data)]\n",
    "Y_train = training_data.iloc[:,0]\n",
    "X_test = test_data.iloc[:,1:ncol(training_data)]\n",
    "Y_test = test_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that we have our data, let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMatrix is a internal data structure that used by XGBoost which is optimized for both memory efficiency \n",
    "# and training speed. \n",
    "dtrain = xgb.DMatrix(X_train, Y_train)\n",
    "\n",
    "# Train the XGBoost model\n",
    "param = { 'max_depth':20, 'silent':1 }\n",
    "num_round = 20\n",
    "xgModel = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict the returns for the S&P 500 ETF in both the train and test periods. If the model is successful, what should the train and test accuracies look like? What would be a key sign that the model has overfit the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: Before you run the next cell, write down what you expect to see if the model is overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overfit model will have low error on the training set, but high error on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions on the test data\n",
    "preds_train = xgModel.predict(xgb.DMatrix(X_train))\n",
    "preds_test = xgModel.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly look at the mean squared error of the predictions on the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6237099209080407e-06"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Calculate the mean squared error on the training set\n",
    "msetrain = sum((preds_train-Y_train)**2)/len(preds_train)\n",
    "msetrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.711855498216044e-05"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Calculate the mean squared error on the test set\n",
    "msetest = sum((preds_test-Y_test)**2)/len(preds_test)\n",
    "msetest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the mean squared error on the test set is an order of magnitude greater than on the training set. Not a good sign. Now let's do some quick calculations to gauge how this would translate into performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combine prediction arrays into a single list\n",
    "predictions = c(preds_train, preds_test)\n",
    "responses = c(Y_train, Y_test)\n",
    "\n",
    "# as a holding size, we'll take predicted return divided by return variance\n",
    "# this is mean-variance optimization with a single asset\n",
    "vols = vol_df.loc[K:n,'vol']\n",
    "position_size = predictions / vols ** 2\n",
    "\n",
    "# TODO: Calculate pnl. Pnl in each time period is holding * realized return.\n",
    "performance = position_size * responses\n",
    "\n",
    "# plot simulated performance\n",
    "plt.plot(np.cumsum(performance))\n",
    "plt.ylabel('Simulated Performance')\n",
    "plt.axvline(x=breakpoint, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simulated returns accumulate throughout the training period, but they are absolutely flat in the testing period. The model has no predictive power whatsoever in the out-of-sample period.\n",
    "\n",
    "Can you think of a few reasons our simulation of performance is unrealistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Answer the above question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We left out any accounting of trading costs. If we had included trading costs, the performance in the out-of-sample period would be downward!\n",
    "\n",
    "2. We didn't account for any time for trading. It's most conservative to assume that we would make trades on the day following our calculation of position size to take, and realize returns the day after that, such that there's a two-day delay between holding size calculation and realized return."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
