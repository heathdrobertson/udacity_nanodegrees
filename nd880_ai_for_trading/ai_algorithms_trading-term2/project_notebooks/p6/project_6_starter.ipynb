{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6: Analyzing Stock Sentiment from Twits\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When deciding the value of a company, it's important to follow the news. For example, a product recall or natural disaster in a company's product chain. You want to be able to turn this information into a signal. Currently, the best tool for the job is a Neural Network. \n",
    "\n",
    "For this project, you'll use posts from the social media site [StockTwits](https://en.wikipedia.org/wiki/StockTwits). The community on StockTwits is full of investors, traders, and entrepreneurs. Each message posted is called a Twit. This is similar to Twitter's version of a post, called a Tweet. You'll build a model around these twits that generate a sentiment score.\n",
    "\n",
    "We've collected a bunch of twits, then hand labeled the sentiment of each. To capture the degree of sentiment, we'll use a five-point scale: very negative, negative, neutral, positive, very positive. Each twit is labeled -2 to 2 in steps of 1, from very negative to very positive respectively. You'll build a sentiment analysis model that will learn to assign sentiment to twits on its own, using this labeled data.\n",
    "\n",
    "The first thing we should to do, is load the data.\n",
    "\n",
    "## Import Twits \n",
    "### Load Twits Data \n",
    "This JSON file contains a list of objects for each twit in the `'data'` field:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Neutral twit body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy twit body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "The fields represent the following:\n",
    "\n",
    "* `'message_body'`: The text of the twit.\n",
    "* `'sentiment'`: Sentiment score for the twit, ranges from -2 to 2 in steps of 1, with 0 being neutral.\n",
    "\n",
    "\n",
    "To see what the data look like by printing the first 10 twits from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$FITB great buy at 26.00...ill wait',\n",
      "  'sentiment': 2,\n",
      "  'timestamp': '2018-07-01T00:00:09Z'},\n",
      " {'message_body': '@StockTwits $MSFT',\n",
      "  'sentiment': 1,\n",
      "  'timestamp': '2018-07-01T00:00:42Z'},\n",
      " {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a '\n",
      "                  'rating of Hold setting target price at USD 350.00. Our own '\n",
      "                  'verdict is Buy  http://www.stocktargetadvisor.com/toprating',\n",
      "  'sentiment': 2,\n",
      "  'timestamp': '2018-07-01T00:01:24Z'},\n",
      " {'message_body': '$AMD I heard there’s a guy who knows someone who thinks '\n",
      "                  'somebody knows something - on StockTwits.',\n",
      "  'sentiment': 1,\n",
      "  'timestamp': '2018-07-01T00:01:47Z'},\n",
      " {'message_body': '$AMD reveal yourself!',\n",
      "  'sentiment': 0,\n",
      "  'timestamp': '2018-07-01T00:02:13Z'},\n",
      " {'message_body': '$AAPL Why the drop? I warren Buffet taking out his '\n",
      "                  'position?',\n",
      "  'sentiment': 1,\n",
      "  'timestamp': '2018-07-01T00:03:10Z'},\n",
      " {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention '\n",
      "                  'https://dividendbot.com?s=BA',\n",
      "  'sentiment': -2,\n",
      "  'timestamp': '2018-07-01T00:04:09Z'},\n",
      " {'message_body': '$BAC ok good we&#39;re not dropping in price over the '\n",
      "                  'weekend, lol',\n",
      "  'sentiment': 1,\n",
      "  'timestamp': '2018-07-01T00:04:17Z'},\n",
      " {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.',\n",
      "  'sentiment': 2,\n",
      "  'timestamp': '2018-07-01T00:08:01Z'},\n",
      " {'message_body': '$GME 3% drop per week after spike... if no news in 3 '\n",
      "                  'months, back to 12s... if BO, then bingo... what is the '\n",
      "                  'odds?',\n",
      "  'sentiment': -2,\n",
      "  'timestamp': '2018-07-01T00:09:03Z'},\n",
      " {'message_body': '$SBUX STRONG BUY!',\n",
      "  'sentiment': 2,\n",
      "  'timestamp': '2018-07-01T00:09:26Z'},\n",
      " {'message_body': '$SNPS short ratio is 2.17 at 2018-06-15 and short % to '\n",
      "                  'float is 1.42% http://sunshineavenue.com/stock/SNPS/ via '\n",
      "                  '@sunshineave',\n",
      "  'sentiment': -2,\n",
      "  'timestamp': '2018-07-01T00:09:36Z'},\n",
      " {'message_body': '$NFLX price squeezing,perfect place for an option straddle '\n",
      "                  'near the supporting trend',\n",
      "  'sentiment': 2,\n",
      "  'timestamp': '2018-07-01T00:12:58Z'},\n",
      " {'message_body': '@DEEPAKM2013 @Nytunes Start of new Q on Monday. Expect '\n",
      "                  'strong buy volume across key companies of various sectors. '\n",
      "                  '$AMZN $AAPL',\n",
      "  'sentiment': 2,\n",
      "  'timestamp': '2018-07-01T00:13:57Z'},\n",
      " {'message_body': '[BREAKOUT Strategy] Current Portfolio : '\n",
      "                  '$ZBRA,$WEB,$TIF,$SRE,$SPPI,$OTEX,$OMF,$NVGS,$NRCIB,$MSG,$KIRK,$GHDX,$FBNK,$ESND,$DRI,$DKS,$CVTI,$CV',\n",
      "  'sentiment': 2,\n",
      "  'timestamp': '2018-07-01T00:14:19Z'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'twits.json'), 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "pprint.pprint(twits['data'][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Data\n",
    "Now let's look at the number of twits in dataset. Print the number of twits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548010\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print out the number of twits\"\"\"\n",
    "\n",
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$FITB great buy at 26.00...ill wait',\n",
      " '@StockTwits $MSFT',\n",
      " '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold '\n",
      " 'setting target price at USD 350.00. Our own verdict is Buy  '\n",
      " 'http://www.stocktargetadvisor.com/toprating',\n",
      " '$AMD I heard there’s a guy who knows someone who thinks somebody knows '\n",
      " 'something - on StockTwits.',\n",
      " '$AMD reveal yourself!',\n",
      " '$AAPL Why the drop? I warren Buffet taking out his position?',\n",
      " '$BA bears have 1 reason on 06-29 to pay more attention '\n",
      " 'https://dividendbot.com?s=BA',\n",
      " '$BAC ok good we&#39;re not dropping in price over the weekend, lol',\n",
      " '$AMAT - Daily Chart, we need to get back to above 50.',\n",
      " '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... '\n",
      " 'if BO, then bingo... what is the odds?',\n",
      " '$SBUX STRONG BUY!',\n",
      " '$SNPS short ratio is 2.17 at 2018-06-15 and short % to float is 1.42% '\n",
      " 'http://sunshineavenue.com/stock/SNPS/ via @sunshineave',\n",
      " '$NFLX price squeezing,perfect place for an option straddle near the '\n",
      " 'supporting trend',\n",
      " '@DEEPAKM2013 @Nytunes Start of new Q on Monday. Expect strong buy volume '\n",
      " 'across key companies of various sectors. $AMZN $AAPL',\n",
      " '[BREAKOUT Strategy] Current Portfolio : '\n",
      " '$ZBRA,$WEB,$TIF,$SRE,$SPPI,$OTEX,$OMF,$NVGS,$NRCIB,$MSG,$KIRK,$GHDX,$FBNK,$ESND,$DRI,$DKS,$CVTI,$CV']\n",
      "[4, 3, 4, 3, 2, 3, 0, 3, 4, 0, 4, 0, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(messages[:15])\n",
    "pprint.pprint(sentiments[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "With our data in hand we need to preprocess our text. These twits are collected by filtering on ticker symbols where these are denoted with a leader $ symbol in the twit itself. For example,\n",
    "\n",
    "`{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}`\n",
    "\n",
    "The ticker symbols don't provide information on the sentiment, and they are in every twit, so we should remove them. This twit also has the `@google` username, again not providing sentiment information, so we should also remove it. We also see a URL `http://t.co/sptHOAh8`. Let's remove these too.\n",
    "\n",
    "The easiest way to remove specific words or phrases is with regex using the `re` module. You can sub out specific patterns with a space:\n",
    "\n",
    "```python\n",
    "re.sub(pattern, ' ', text)\n",
    "```\n",
    "This will substitute a space with anywhere the pattern matches in the text. Later when we tokenize the text, we'll split appropriately on those spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['breakout', 'strategy', 'current', 'portfolio']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    text = message.lower()\n",
    "\n",
    "    url_pattern = re.compile(\n",
    "        r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    text = re.sub(url_pattern, ' ', text)\n",
    "    \n",
    "    symb_pattern = re.compile(r'[@#$][\\w_-]+', re.IGNORECASE)\n",
    "    text = re.sub(symb_pattern, ' ', text)\n",
    "    \n",
    "    not_alph_pattern = re.compile(r'[\\W\\d]')\n",
    "    text = re.sub(not_alph_pattern, ' ', text)\n",
    "    \n",
    "    single_char = re.compile(r'\\s[\\w]\\s')\n",
    "    text = re.sub(single_char, ' ', text)\n",
    "    \n",
    "    tknizr = TweetTokenizer()\n",
    "    tokenized_words = tknizr.tokenize(text)\n",
    "    tokens = [w for w in tokenized_words]\n",
    "    \n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w) for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# 13\n",
    "# @DEEPAKM2013 @Nytunes Start of new Q on Monday. Expect strong buy volume across key companies of various sectors. $AMZN $AAPL\n",
    "# 14\n",
    "# [BREAKOUT Strategy] Current Portfolio : $ZBRA,$WEB,$TIF,$SRE,$SPPI,$OTEX,$OMF,$NVGS,$NRCIB,$MSG,$KIRK,$GHDX,$FBNK,$ESND,$DRI,$DKS,$CVTI,$CV\n",
    "\n",
    "msg = messages[14]\n",
    "preprocess(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess All the Twits \n",
    "Now we can preprocess each of the twits in our dataset. Apply the function `preprocess` to all the twit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "\n",
    "for twit in messages:\n",
    "    tokenized.append(preprocess(twit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great', 'buy', 'at', 'ill', 'wait'], [], ['for', 'jefferies', 'maintains', 'with', 'rating', 'of', 'hold', 'setting', 'target', 'price', 'at', 'usd', 'our', 'own', 'verdict', 'is', 'buy'], ['heard', 'there', 'guy', 'who', 'know', 'someone', 'who', 'think', 'somebody', 'know', 'something', 'on', 'stocktwits'], ['reveal', 'yourself'], ['why', 'the', 'drop', 'warren', 'buffet', 'taking', 'out', 'his', 'position'], ['bear', 'have', 'reason', 'on', 'to', 'pay', 'more', 'attention'], ['ok', 'good', 'we', 're', 'not', 'dropping', 'in', 'price', 'over', 'the', 'weekend', 'lol'], ['daily', 'chart', 'we', 'need', 'to', 'get', 'back', 'to', 'above'], ['drop', 'per', 'week', 'after', 'spike', 'if', 'no', 'news', 'in', 'month', 'back', 'to', 'if', 'bo', 'then', 'bingo', 'what', 'is', 'the', 'odds']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "Now with all of our messages tokenized, we want to create a vocabulary and count up how often each word appears in our entire corpus. Use the [`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter) function to count up all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "\n",
    "for msg in tokenized:\n",
    "    words += msg\n",
    "    \n",
    "def bag_of_words(words):\n",
    "    \"\"\"Create a vocabulary by using Bag of words\n",
    "    \"\"\"\n",
    "    bow = Counter()\n",
    "    \n",
    "    for word in words:\n",
    "        bow[word] += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "bow = bag_of_words(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Words Appearing in Message\n",
    "With our vocabulary, now we'll remove some of the most common words such as 'the', 'and', 'it', etc. These words don't contribute to identifying sentiment and are really common, resulting in a lot of noise in our input. If we can filter these out, then our network should have an easier time learning.\n",
    "\n",
    "We also want to remove really rare words that show up in a only a few twits. Here you'll want to divide the count of each word by the number of messages. Then remove words that only appear in some small fraction of the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------\n",
      "Length Tokenized Twits:1548010\n",
      "\n",
      "['great', 'buy', 'at', 'ill', 'wait']\n",
      "\n",
      "[]\n",
      "\n",
      "['for', 'jefferies', 'maintains', 'with', 'rating', 'of', 'hold', 'setting', 'target', 'price', 'at', 'usd', 'our', 'own', 'verdict', 'is', 'buy']\n",
      "\n",
      "['heard', 'there', 'guy', 'who', 'know', 'someone', 'who', 'think', 'somebody', 'know', 'something', 'on', 'stocktwits']\n",
      "\n",
      "['reveal', 'yourself']\n",
      "\n",
      "['why', 'the', 'drop', 'warren', 'buffet', 'taking', 'out', 'his', 'position']\n",
      "\n",
      "['bear', 'have', 'reason', 'on', 'to', 'pay', 'more', 'attention']\n",
      "\n",
      "['ok', 'good', 'we', 're', 'not', 'dropping', 'in', 'price', 'over', 'the', 'weekend', 'lol']\n",
      "\n",
      "['daily', 'chart', 'we', 'need', 'to', 'get', 'back', 'to', 'above']\n",
      "\n",
      "['drop', 'per', 'week', 'after', 'spike', 'if', 'no', 'news', 'in', 'month', 'back', 'to', 'if', 'bo', 'then', 'bingo', 'what', 'is', 'the', 'odds']\n",
      "\n",
      "---------------\n",
      "Length BOW:105283\n",
      "17570\n"
     ]
    }
   ],
   "source": [
    "# Printing to check data structure.\n",
    "\n",
    "len_tokenized = len(tokenized)\n",
    "len_bow = len(bow)\n",
    "spcr = '\\n' + '-'*15 + '\\n'\n",
    "\n",
    "print(f'{spcr}Length Tokenized Twits:{len_tokenized}')\n",
    "\n",
    "for ndx, twit in enumerate(tokenized[:10]):\n",
    "    print(f'\\n{twit}')\n",
    "great = bow['great']\n",
    "print(f'{spcr}Length BOW:{len_bow}\\n{great}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548010\n",
      "\n",
      "---------------\n",
      "[('the', 398828), ('to', 379789), ('amp', 295510), ('is', 284902), ('for', 273569), ('on', 241672), ('of', 211367), ('and', 208527), ('in', 205367), ('this', 203576)]\n",
      "\n",
      "48577\n",
      "\n",
      "---------------\n",
      "['great', 'buy', 'at', 'ill', 'wait', 'for', 'jefferies', 'maintains', 'with', 'rating', 'of', 'hold', 'setting', 'target', 'price', 'usd', 'our', 'own', 'verdict', 'is', 'heard', 'there', 'guy', 'who', 'know', 'someone', 'think', 'somebody', 'something', 'on', 'stocktwits', 'reveal', 'yourself', 'why', 'the', 'drop', 'warren', 'buffet', 'taking', 'out', 'his', 'position', 'bear', 'have', 'reason', 'to', 'pay', 'more', 'attention', 'ok']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs = Dictionart that contains the Frequency of words appearing in messages. \\\n",
    "            The key is the token and the value is the frequency of that word in the corpus.\n",
    "    low_cutoff = Float that is the frequency cutoff. Drop words with a frequency that is lower or equal \\\n",
    "            to this number.\n",
    "    high_cutoff = Integer that is the cut off for most common words. Drop words that are the `high_cutoff` \\\n",
    "            most common words.\n",
    "    K_most_common = The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "\"\"\"\n",
    "\n",
    "total_count = len(bow)\n",
    "freqs = {word: count/total_count for word, count in bow.items()}\n",
    "\n",
    "low_cutoff = 1e-5\n",
    "high_cutoff = 10\n",
    "K_most_common = bow.most_common(high_cutoff)\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "print(len(tokenized))\n",
    "print(f'{spcr}{K_most_common}\\n')\n",
    "print(len(filtered_words)) \n",
    "print(f'{spcr}{filtered_words[:50]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Vocabulary by Removing Filtered Words\n",
    "Let's creat three variables that will help with our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab = {}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {}\n",
    "\n",
    "for word_id, word in enumerate(filtered_words, 1):\n",
    "    vocab[word] = word_id\n",
    "    id2vocab[word_id] = word\n",
    "\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "filtered = [[word for word in tknzd_msg if word in filtered_words] for tknzd_msg in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the classes\n",
    "Let's do a few last pre-processing steps. If we look at how our twits are labeled, we'll find that 50% of them are neutral. This means that our network will be 50% accurate just by guessing 0 every single time. To help our network learn appropriately, we'll want to balance our classes.\n",
    "That is, make sure each of our different sentiment scores show up roughly as frequently in the data.\n",
    "\n",
    "What we can do here is go through each of our examples and randomly drop twits with neutral sentiment. What should be the probability we drop these twits if we want to get around 20% neutral twits starting at 50% neutral? We should also take this opportunity to remove messages with length 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it correctly, you should see the following result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19523199060467847"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------\n",
    "**⬇ CAN BE REMOVED ⬇**\n",
    "\n",
    "### Notebook Objects\n",
    "__Reloads Notebook Pyhton Objects__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hydrator(d):\n",
    "    \"\"\"Keeps data fresh.\"\"\"\n",
    "    with open('data.pickle', 'wb+') as f:\n",
    "        pickle.dump(d, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'messages': messages,\n",
    "    'sentiments': sentiments,\n",
    "    'tokenized': tokenized,\n",
    "    'bow': bow,\n",
    "    'filtered_words': filtered_words,\n",
    "    'vocab': vocab,\n",
    "    'id2vocab': id2vocab,\n",
    "    'filtered': filtered,\n",
    "    'balanced': balanced,\n",
    "    'token_ids': token_ids,\n",
    "}\n",
    "\n",
    "# hydrator(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rehydrate_proj():\n",
    "    \"\"\"Keeps data fresh, without rerunning everythign above.\"\"\"\n",
    "    with open('data.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        messages = data['messages']\n",
    "        sentiments = data['sentiments']\n",
    "        tokenized = data['tokenized']\n",
    "        bow = data['bow']\n",
    "        filtered_words = data['filtered_words']\n",
    "        vocab = data['vocab']\n",
    "        id2vocab = data['id2vocab']\n",
    "        filtered = data['filtered']\n",
    "        balanced = data['balanced']\n",
    "        token_ids = data['token_ids']\n",
    "        return messages, sentiments, tokenized, bow, filtered_words, vocab, id2vocab, filtered, balanced, token_ids\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages, sentiments, tokenized, bow, filtered_words, vocab, id2vocab, filtered, balanced, token_ids = rehydrate_proj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⬆ CAN BE REMOVED ⬆**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Now we have our vocabulary which means we can transform our tokens into ids, which are then passed to our network. So, let's define the network now!\n",
    "\n",
    "Here is a nice diagram showing the network we'd like to build: \n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "### Implement the text classifier\n",
    "Before we build text classifier, if you remember from the other network that you built in  \"Sentiment Analysis with an RNN\"  exercise  - which there, the network called \" SentimentRNN\", here we named it \"TextClassifer\" - consists of three main parts: 1) init function `__init__` 2) forward pass `forward`  3) hidden state `init_hidden`. \n",
    "\n",
    "This network is pretty similar to the network you built expect in the  `forward` pass, we use softmax instead of sigmoid. The reason we are not using sigmoid is that the output of NN is not a binary. In our network, sentiment scores have 5 possible outcomes. We are looking for an outcome with the highest probability thus softmax is a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # TODO Implement\n",
    "\n",
    "        # Setup embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Setup additional layers\n",
    "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers,\n",
    "                            dropout=dropout, batch_first=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        self.logsm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        # embeddings and lstm_out\n",
    "        nn_input = nn_input.long()\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden_state)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Softmax function\n",
    "        logps = self.logsm(out)\n",
    "\n",
    "        batch_size = nn_input.size(1)\n",
    "        \n",
    "        return logps, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4987, -1.4484, -1.5218, -1.5268, -2.2434],\n",
      "        [-1.5015, -1.4520, -1.5142, -1.5303, -2.2382],\n",
      "        [-1.5079, -1.4349, -1.5332, -1.5189, -2.2476],\n",
      "        [-1.4943, -1.4554, -1.5128, -1.5255, -2.2584]])\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(vocab_size=len(vocab), embed_size=10, lstm_size=6, output_size=5, lstm_layers=2, dropout=0.1)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### DataLoaders and Batching\n",
    "Now we should build a generator that we can use to loop through our data. It'll be more efficient if we can pass our sequences in as batches. Our input tensors should look like `(sequence_length, batch_size)`. So if our sequences are 40 tokens long and we pass in 25 sequences, then we'd have an input size of `(40, 25)`.\n",
    "\n",
    "If we set our sequence length to 40, what do we do with messages that are more or less than 40 tokens? For messages with fewer than 40 tokens, we will pad the empty spots with zeros. We should be sure to **left** pad so that the RNN starts from nothing before going through the data. If the message has 20 tokens, then the first 20 spots of our 40 long sequence will be 0. If a message has more than 40 tokens, we'll just keep the first 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and  Validation\n",
    "With our data in nice shape, we'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "split_frac = 0.8\n",
    "\n",
    "split_idx = int(len(token_ids)*split_frac)\n",
    "train_features, valid_features = token_ids[:split_idx], token_ids[split_idx:]\n",
    "train_labels, valid_labels = sentiments[:split_idx], sentiments[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "model = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "hidden = model.init_hidden(64)\n",
    "logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "It's time to train the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(48578, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (logsm): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:101 | Training Loss:0.984: 101it [00:24,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:100 | Validation Loss:0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:200 | Training Loss:0.908: 200it [00:46,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:200 | Validation Loss:0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:301 | Training Loss:0.890: 301it [01:09,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:300 | Validation Loss:0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:401 | Training Loss:0.803: 401it [01:32,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:400 | Validation Loss:0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:500 | Training Loss:0.792: 500it [01:55,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:500 | Validation Loss:0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:601 | Training Loss:0.770: 601it [02:19,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:600 | Validation Loss:0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:700 | Training Loss:0.720: 700it [02:42,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:700 | Validation Loss:0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:801 | Training Loss:0.737: 801it [03:05,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:800 | Validation Loss:0.594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:900 | Training Loss:0.744: 900it [03:28,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:900 | Validation Loss:0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:1000 | Training Loss:0.654: 1000it [03:52,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:1000 | Validation Loss:0.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:1100 | Training Loss:0.743: 1100it [04:15,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:1100 | Validation Loss:0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:1200 | Training Loss:0.703: 1200it [04:38,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:1200 | Validation Loss:0.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:1301 | Training Loss:0.686: 1301it [05:01,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:1300 | Validation Loss:0.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:1401 | Training Loss:0.707: 1401it [05:25,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:1400 | Validation Loss:0.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:1500 | Training Loss:0.617: 1500it [05:48,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:1500 | Validation Loss:0.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:1600 | Training Loss:0.729: 1600it [06:11,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/3 Batch:1600 | Validation Loss:0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Batch:1612 | Training Loss:0.707: 1612it [06:14,  4.52it/s]\n",
      "Epoch:2 | Batch:100 | Training Loss:0.662: 100it [00:24,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:100 | Validation Loss:0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:200 | Training Loss:0.643: 200it [00:47,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:200 | Validation Loss:0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:300 | Training Loss:0.689: 300it [01:11,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:300 | Validation Loss:0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:400 | Training Loss:0.628: 400it [01:34,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:400 | Validation Loss:0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:501 | Training Loss:0.746: 501it [01:57,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:500 | Validation Loss:0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:600 | Training Loss:0.643: 600it [02:21,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:600 | Validation Loss:0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:701 | Training Loss:0.707: 701it [02:44,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:700 | Validation Loss:0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:800 | Training Loss:0.668: 800it [03:07,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:800 | Validation Loss:0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:900 | Training Loss:0.667: 900it [03:30,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:900 | Validation Loss:0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:1000 | Training Loss:0.618: 1000it [03:54,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:1000 | Validation Loss:0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:1101 | Training Loss:0.617: 1101it [04:17,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:1100 | Validation Loss:0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:1200 | Training Loss:0.573: 1200it [04:40,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:1200 | Validation Loss:0.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:1300 | Training Loss:0.638: 1300it [05:04,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:1300 | Validation Loss:0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:1400 | Training Loss:0.589: 1400it [05:27,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:1400 | Validation Loss:0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:1500 | Training Loss:0.646: 1500it [05:50,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:1500 | Validation Loss:0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:1601 | Training Loss:0.594: 1601it [06:14,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/3 Batch:1600 | Validation Loss:0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Batch:1612 | Training Loss:0.607: 1612it [06:16,  4.47it/s]\n",
      "Epoch:3 | Batch:100 | Training Loss:0.559: 100it [00:24,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:100 | Validation Loss:0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:200 | Training Loss:0.572: 200it [00:47,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:200 | Validation Loss:0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:301 | Training Loss:0.564: 301it [01:11,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:300 | Validation Loss:0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:401 | Training Loss:0.512: 401it [01:34,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:400 | Validation Loss:0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:501 | Training Loss:0.589: 501it [01:57,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:500 | Validation Loss:0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:600 | Training Loss:0.530: 600it [02:20,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:600 | Validation Loss:0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:700 | Training Loss:0.586: 700it [02:44,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:700 | Validation Loss:0.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:800 | Training Loss:0.577: 800it [03:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:800 | Validation Loss:0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:900 | Training Loss:0.642: 900it [03:30,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:900 | Validation Loss:0.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:1000 | Training Loss:0.626: 1000it [03:54,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:1000 | Validation Loss:0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:1100 | Training Loss:0.606: 1100it [04:17,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:1100 | Validation Loss:0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:1201 | Training Loss:0.623: 1201it [04:41,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:1200 | Validation Loss:0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:1300 | Training Loss:0.623: 1300it [05:04,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:1300 | Validation Loss:0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:1400 | Training Loss:0.537: 1400it [05:27,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:1400 | Validation Loss:0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:1500 | Training Loss:0.580: 1500it [05:50,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:1500 | Validation Loss:0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:1600 | Training Loss:0.557: 1600it [06:14,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/3 Batch:1600 | Validation Loss:0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Batch:1612 | Training Loss:0.604: 1612it [06:16,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "clip = 5\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    steps = 0\n",
    "    \n",
    "    tqdm_dataloader = tqdm(\n",
    "        dataloader(train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True)\n",
    "    )\n",
    "    \n",
    "    for text_batch, labels in tqdm_dataloader:\n",
    "        steps += 1\n",
    "        hidden = model.init_hidden(labels.shape[0])\n",
    "        \n",
    "        # Set Device\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # TODO Implement: Train Model\n",
    "        \n",
    "        # ------------------------------\n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, hidden = model(text_batch, hidden)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        # ------------------------------\n",
    "        tqdm_dataloader.set_description(f'Epoch:{epoch + 1} | Batch:{steps} | Training Loss:{loss:.3f}')\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # TODO Implement: Print metrics\n",
    "            # Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "            # ------------------------------\n",
    "            validation_losses = []        \n",
    "            \n",
    "            \n",
    "            for inputs, labels in dataloader(valid_features, valid_labels, batch_size=batch_size):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "            valid_hidden = model.init_hidden(labels.shape[0])\n",
    "            \n",
    "            for each in valid_hidden:\n",
    "                each.to(device)\n",
    "            output, valid_hidden = model(inputs, valid_hidden)\n",
    "            \n",
    "            validation_loss = criterion(output.squeeze(), labels)\n",
    "            validation_losses.append(validation_loss.item())\n",
    "            \n",
    "            model.train()\n",
    "            # ------------------------------\n",
    "            tqdm.write(\n",
    "                f'Epoch:{epoch + 1}/{epochs} Batch:{steps} | Validation Loss:{np.mean(validation_losses):.3f}'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "### Prediction \n",
    "Okay, now that you have a trained model, try it on some new twits and see if it works appropriately. Remember that for any new text, you'll need to preprocess it first before passing it to the network. Implement the `predict` function to generate the prediction vector from a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"    \n",
    "    \n",
    "    # TODO Implement\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    \n",
    "    # Filter non-vocab words\n",
    "    tokens = [word for word in tokens if word in vocab]\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[word] if word is not None else \"sell\" for word in tokens]\n",
    "        \n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.FloatTensor(tokens).view(-1, 1)))\n",
    "    # Get the NN output\n",
    "    hidden = model.init_hidden(1)\n",
    "    logps, _ = model.forward(text_input, hidden)\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.exp(logps)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0001,  0.0084,  0.0036,  0.8727,  0.1152]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: What is the prediction of the model? What is the uncertainty of the prediction?\n",
    "** TODO: Answer Question**\n",
    "\n",
    "Model scores or output are it's predicted probability distribution, all values adding to one, of the given message to the sentiment classes.  Each score or probabilty is the confidence of the model that the twit sentiment falls within that sentiment class.  For this twit, the prediction of the model is that the sentiment of the twit is positive with a score of 0.89667 in the second to last class position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model and we can make predictions. We can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now we have a stream of twits. For each of those twits, pull out the stocks mentioned in them and keep track of the sentiments. Remember that in the twits, ticker symbols are encoded with a dollar sign as the first character, all caps, and 2-4 letters, like $AAPL. Ideally, you'd want to track the sentiments of the stocks in your universe and use this as a signal in your larger model(s).\n",
    "\n",
    "## Testing\n",
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$ORLY break 280 then free fall from there',\n",
      "  'timestamp': '2018-11-01T00:30:48Z'},\n",
      " {'message_body': '@MapTechnicalForecasting But what will S&amp;P500 be at Fri '\n",
      "                  'lunchtime after $AAPL reports Thurs afternoon? That is the '\n",
      "                  'more relevant question',\n",
      "  'timestamp': '2018-11-01T00:30:55Z'},\n",
      " {'message_body': '$EBAY earnings move  5.9% vs an expected ±9.7% move  '\n",
      "                  'http://tinyurl.com/y7rldku9',\n",
      "  'timestamp': '2018-11-01T00:30:59Z'},\n",
      " {'message_body': '$CRM max pain is 139 for expiry 2018-11-02 Source: '\n",
      "                  'http://sweep.ly/maxpain.html',\n",
      "  'timestamp': '2018-11-01T00:31:11Z'},\n",
      " {'message_body': '$GE Firm market cap about $88B.  Need to time 25-1 reverse '\n",
      "                  'split when she hits $8.75 this Q.',\n",
      "  'timestamp': '2018-11-01T00:31:15Z'},\n",
      " {'message_body': '$ABMD I’m afraid to hold during earning so sold both call '\n",
      "                  'and put today: will buy back after earning look at $isrg as '\n",
      "                  'a study case after war',\n",
      "  'timestamp': '2018-11-01T00:31:33Z'},\n",
      " {'message_body': '$BAC Banking sector is about to soar. Banks spends so much '\n",
      "                  'money trying to comply with the rules that were placed '\n",
      "                  'after the financial crisis',\n",
      "  'timestamp': '2018-11-01T00:32:17Z'},\n",
      " {'message_body': '$BAC back in for like no ride now...',\n",
      "  'timestamp': '2018-11-01T00:32:31Z'},\n",
      " {'message_body': '$AMD $DJIA  Happy Halloween From THE STAGGERRRRR......\\n'\n",
      "                  ' https://www.youtube.com/watch?v=2x-4BAqk4wc',\n",
      "  'timestamp': '2018-11-01T00:32:32Z'},\n",
      " {'message_body': '$ORLY plunge to 220 to fill gap',\n",
      "  'timestamp': '2018-11-01T00:32:51Z'},\n",
      " {'message_body': '$YUM reported -33.33% YoY growth in Same Store Sales for '\n",
      "                  'Q3.\\n'\n",
      "                  'http://www.estimize.com/intro/yum?chart=historical&amp;metric_name=same_store_sales&amp;utm_content=YUM&amp;utm_medium=metric_growth&amp;utm_source=stocktwits',\n",
      "  'timestamp': '2018-11-01T00:33:09Z'},\n",
      " {'message_body': '$CSCO max pain is 45.5 for expiry 2018-11-02 Source: '\n",
      "                  'http://sweep.ly/maxpain.html',\n",
      "  'timestamp': '2018-11-01T00:33:30Z'},\n",
      " {'message_body': '$AGN is cheaper than 92% of the companies listed in the '\n",
      "                  'same industry. '\n",
      "                  'https://www.chartmill.com/analyze.php?utm_source=stocktwits&amp;utm_medium=FA&amp;utm_content=VALUATION&amp;utm_campaign=social_tracking#/AGN?r=fa&amp;key=a0ee423d-0ce9-41af-9088-9c0720c7ccd9',\n",
      "  'timestamp': '2018-11-01T00:33:59Z'},\n",
      " {'message_body': 'Huntington Bancshares Director Richard Neu Buys $93,690.00 '\n",
      "                  'in $HBAN https://www.marketbeat.com/i/303249',\n",
      "  'timestamp': '2018-11-01T00:34:16Z'},\n",
      " {'message_body': '$AAPL devil&#39;s advocate. Why would this go down?',\n",
      "  'timestamp': '2018-11-01T00:34:55Z'},\n",
      " {'message_body': '$AMZN happy Halloween', 'timestamp': '2018-11-01T00:35:10Z'},\n",
      " {'message_body': '$AAPL china tariff issue will impact their earnings '\n",
      "                  'negatively. News will be significant enough to thrash the '\n",
      "                  'entire stock mkt',\n",
      "  'timestamp': '2018-11-01T00:35:11Z'},\n",
      " {'message_body': '@w97 ssshhhhh.  Bulls never acknowledge the possibility '\n",
      "                  'they might be wrong.  The $MU cult is quite disturbing',\n",
      "  'timestamp': '2018-11-01T00:35:19Z'},\n",
      " {'message_body': '$CSX max pain is 67.5 for expiry 2018-11-02 Source: '\n",
      "                  'http://sweep.ly/maxpain.html',\n",
      "  'timestamp': '2018-11-01T00:35:20Z'},\n",
      " {'message_body': '$CTAS max pain is 190 for expiry 2018-11-16 Source: '\n",
      "                  'http://sweep.ly/maxpain.html',\n",
      "  'timestamp': '2018-11-01T00:35:30Z'},\n",
      " {'message_body': '$SIG Bearish earnings reversal should drop to it&#39;s 5 '\n",
      "                  'EMA  two points lower.',\n",
      "  'timestamp': '2018-11-01T00:35:31Z'},\n",
      " {'message_body': 'Big Trade Blocks for $ANTM (Largest Trade: 0.59% of Volume) '\n",
      "                  'https://icebergbot.com?s=ANTM',\n",
      "  'timestamp': '2018-11-01T00:35:35Z'},\n",
      " {'message_body': 'Today&#39;s insight on $NE '\n",
      "                  'https://app.seasonaledge.com/tools/I/NE',\n",
      "  'timestamp': '2018-11-01T00:35:40Z'},\n",
      " {'message_body': '$DWDP I had thought this co could out compete rest of the '\n",
      "                  'world by making high qual things from plastic made by cheap '\n",
      "                  'natural  gas ?',\n",
      "  'timestamp': '2018-11-01T00:35:45Z'},\n",
      " {'message_body': '$UNH GOOGLE :OPTUM, ATTORNEY MARK CUKER PHARMACY FRAUD '\n",
      "                  'PENSYLVANNIA',\n",
      "  'timestamp': '2018-11-01T00:35:47Z'},\n",
      " {'message_body': '$GS Yuan Will Hit 7 as China Avoids Heavy Intervention, '\n",
      "                  'Goldman Says via http://www.hvper.com/?ref=stw',\n",
      "  'timestamp': '2018-11-01T00:36:01Z'},\n",
      " {'message_body': '$BB another car comes off the line w QNX every .7 sec! QNX '\n",
      "                  'growing at 30% YoY QoQ. All players hard code onto their '\n",
      "                  'chips! Yum yum!! $NVDA',\n",
      "  'timestamp': '2018-11-01T00:36:01Z'},\n",
      " {'message_body': '$MCD bulls have 5 reasons on 10-31 to pay more attention '\n",
      "                  'https://dividendbot.com?s=MCD',\n",
      "  'timestamp': '2018-11-01T00:36:05Z'},\n",
      " {'message_body': '$AMZN boom', 'timestamp': '2018-11-01T00:36:09Z'},\n",
      " {'message_body': '$CTL max pain is 20.5 for expiry 2018-11-02 Source: '\n",
      "                  'http://sweep.ly/maxpain.html',\n",
      "  'timestamp': '2018-11-01T00:36:10Z'},\n",
      " {'message_body': 'Today&#39;s insight on $K '\n",
      "                  'https://app.seasonaledge.com/tools/I/K',\n",
      "  'timestamp': '2018-11-01T00:36:53Z'},\n",
      " {'message_body': 'The industry average ROA is -1.59%. $MSFT outperforms 81% '\n",
      "                  'of its industry peers. '\n",
      "                  'https://www.chartmill.com/analyze.php?utm_source=stocktwits&amp;utm_medium=FA&amp;utm_content=PROFITABILITY&amp;utm_campaign=social_tracking#/MSFT?r=fa&amp;key=92318c9b-cb66-4932-8e30-5564fa1dd6a6',\n",
      "  'timestamp': '2018-11-01T00:36:58Z'},\n",
      " {'message_body': '$CLF What a night in New York.  Perfect weather.  Happy '\n",
      "                  'Halloween.  Hope my car doesn’t get egged.',\n",
      "  'timestamp': '2018-11-01T00:37:05Z'},\n",
      " {'message_body': '@shortvolume $WM is a utility like $RSG',\n",
      "  'timestamp': '2018-11-01T00:37:16Z'},\n",
      " {'message_body': '$GE Shows a possibility of hitting $8.75 at 35 year trend '\n",
      "                  'using quarterly candles:',\n",
      "  'timestamp': '2018-11-01T00:37:16Z'},\n",
      " {'message_body': '$CTSH max pain is 72.5 for expiry 2018-11-02 Source: '\n",
      "                  'http://sweep.ly/maxpain.html',\n",
      "  'timestamp': '2018-11-01T00:37:19Z'},\n",
      " {'message_body': '@GoldenWhale @dt100 Tim Cook just posted it on Twitter. '\n",
      "                  'It’s official, $AAPL will buy $mu. IT WILL SKYROCKET '\n",
      "                  'TOMORROW !!!!',\n",
      "  'timestamp': '2018-11-01T00:37:20Z'},\n",
      " {'message_body': '$PKI Of course...', 'timestamp': '2018-11-01T00:37:24Z'},\n",
      " {'message_body': '$WATT I&#39;m thinking diversified  $AMD $TNDM $NIO $JWN '\n",
      "                  'and of course wattup for the long',\n",
      "  'timestamp': '2018-11-01T00:37:30Z'},\n",
      " {'message_body': 'Today&#39;s insight on $CLX '\n",
      "                  'https://app.seasonaledge.com/tools/I/CLX',\n",
      "  'timestamp': '2018-11-01T00:38:06Z'},\n",
      " {'message_body': '$AMZN I don’t give a fukc I bought my shares back today and '\n",
      "                  'gonna roll on with Amazon \\n'\n",
      "                  'Also bought some apple',\n",
      "  'timestamp': '2018-11-01T00:38:23Z'},\n",
      " {'message_body': '$INTC fair price 38 heard it here first',\n",
      "  'timestamp': '2018-11-01T00:38:25Z'},\n",
      " {'message_body': '$FCX charts are telling me upwards tomorrow.',\n",
      "  'timestamp': '2018-11-01T00:38:29Z'},\n",
      " {'message_body': '$AMZN even after ours was green today I’m not sure what to '\n",
      "                  'think up or down\\n'\n",
      "                  '\\n'\n",
      "                  'That they have in store for us',\n",
      "  'timestamp': '2018-11-01T00:38:29Z'},\n",
      " {'message_body': '$AAPL estimates distribution - here’s what 42 Estimize '\n",
      "                  'analysts are expecting $AAPL to report for Q4 Gross Margins '\n",
      "                  'tomorrow 11/01 AMC.\\n'\n",
      "                  'http://www.estimize.com/intro/aapl?chart=scatterplotOrHistogram&amp;metric_name=gross_margins&amp;utm_content=AAPL&amp;utm_medium=metric_distribution&amp;utm_source=stocktwits',\n",
      "  'timestamp': '2018-11-01T00:39:11Z'},\n",
      " {'message_body': '$AAPL estimates distribution - here’s what 30 Estimize '\n",
      "                  'analysts are expecting $AAPL to report for Q4 Mac Sales '\n",
      "                  'tomorrow 11/01 AMC.\\n'\n",
      "                  'http://www.estimize.com/intro/aapl?chart=scatterplotOrHistogram&amp;metric_name=mac_unit_sales&amp;utm_content=AAPL&amp;utm_medium=metric_distribution&amp;utm_source=stocktwits',\n",
      "  'timestamp': '2018-11-01T00:39:11Z'},\n",
      " {'message_body': 'Here’s what 6 Estimize analysts believe $QCOM will report '\n",
      "                  'for Q4 Gross Margins on 11/07 AMC.\\n'\n",
      "                  'http://www.estimize.com/intro/qcom?chart=historical&amp;metric_name=gross_margins&amp;utm_content=QCOM&amp;utm_medium=metric_update&amp;utm_source=stocktwits',\n",
      "  'timestamp': '2018-11-01T00:39:11Z'},\n",
      " {'message_body': '$AAPL estimates distribution - here’s what 64 Estimize '\n",
      "                  'analysts are expecting $AAPL to report for Q4 iPhone Sales '\n",
      "                  'tomorrow 11/01 AMC.\\n'\n",
      "                  'http://www.estimize.com/intro/aapl?chart=scatterplotOrHistogram&amp;metric_name=iphone_unit_sales&amp;utm_content=AAPL&amp;utm_medium=metric_distribution&amp;utm_source=stocktwits',\n",
      "  'timestamp': '2018-11-01T00:39:11Z'},\n",
      " {'message_body': '$AAPL estimates distribution - here’s what 31 Estimize '\n",
      "                  'analysts are expecting $AAPL to report for Q4 iPad Sales '\n",
      "                  'tomorrow 11/01 AMC.\\n'\n",
      "                  'http://www.estimize.com/intro/aapl?chart=scatterplotOrHistogram&amp;metric_name=ipad_unit_sales&amp;utm_content=AAPL&amp;utm_medium=metric_distribution&amp;utm_source=stocktwits',\n",
      "  'timestamp': '2018-11-01T00:39:11Z'},\n",
      " {'message_body': 'Here’s what 11 Estimize analysts believe $SBUX will report '\n",
      "                  'for Q4 Same Store Sales tomorrow 11/01 AMC.\\n'\n",
      "                  'http://www.estimize.com/intro/sbux?chart=historical&amp;metric_name=same_store_sales&amp;utm_content=SBUX&amp;utm_medium=metric_update&amp;utm_source=stocktwits',\n",
      "  'timestamp': '2018-11-01T00:39:12Z'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "    td = test_data['data'][150:200]\n",
    "    pprint.pprint(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twit_stream():\n",
    "    for twit in td:\n",
    "        yield twit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` function, let's apply it to a stream of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" \n",
    "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
    "    \"\"\"\n",
    "    for twit in stream:\n",
    "\n",
    "        # Get the message text\n",
    "        text = twit['message_body']\n",
    "        print(text)\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "        score = predict(text, model, vocab)\n",
    "\n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ORLY break 280 then free fall from there\n",
      "@MapTechnicalForecasting But what will S&amp;P500 be at Fri lunchtime after $AAPL reports Thurs afternoon? That is the more relevant question\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 0.0156,  0.0521,  0.5695,  0.3384,  0.0243]]), 'timestamp': '2018-11-01T00:30:55Z'}\n",
      "-----------------\n",
      "\n",
      "$EBAY earnings move  5.9% vs an expected ±9.7% move  http://tinyurl.com/y7rldku9\n",
      "$CRM max pain is 139 for expiry 2018-11-02 Source: http://sweep.ly/maxpain.html\n",
      "$GE Firm market cap about $88B.  Need to time 25-1 reverse split when she hits $8.75 this Q.\n",
      "$ABMD I’m afraid to hold during earning so sold both call and put today: will buy back after earning look at $isrg as a study case after war\n",
      "$BAC Banking sector is about to soar. Banks spends so much money trying to comply with the rules that were placed after the financial crisis\n",
      "$BAC back in for like no ride now...\n",
      "$AMD $DJIA  Happy Halloween From THE STAGGERRRRR......\n",
      " https://www.youtube.com/watch?v=2x-4BAqk4wc\n",
      "$ORLY plunge to 220 to fill gap\n",
      "$YUM reported -33.33% YoY growth in Same Store Sales for Q3.\n",
      "http://www.estimize.com/intro/yum?chart=historical&amp;metric_name=same_store_sales&amp;utm_content=YUM&amp;utm_medium=metric_growth&amp;utm_source=stocktwits\n",
      "$CSCO max pain is 45.5 for expiry 2018-11-02 Source: http://sweep.ly/maxpain.html\n",
      "$AGN is cheaper than 92% of the companies listed in the same industry. https://www.chartmill.com/analyze.php?utm_source=stocktwits&amp;utm_medium=FA&amp;utm_content=VALUATION&amp;utm_campaign=social_tracking#/AGN?r=fa&amp;key=a0ee423d-0ce9-41af-9088-9c0720c7ccd9\n",
      "Huntington Bancshares Director Richard Neu Buys $93,690.00 in $HBAN https://www.marketbeat.com/i/303249\n",
      "$AAPL devil&#39;s advocate. Why would this go down?\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 0.7455,  0.1666,  0.0274,  0.0596,  0.0009]]), 'timestamp': '2018-11-01T00:34:55Z'}\n",
      "-----------------\n",
      "\n",
      "$AMZN happy Halloween\n",
      "{'symbol': '$AMZN', 'score': tensor([[ 0.0719,  0.0272,  0.3310,  0.4519,  0.1180]]), 'timestamp': '2018-11-01T00:35:10Z'}\n",
      "-----------------\n",
      "\n",
      "$AAPL china tariff issue will impact their earnings negatively. News will be significant enough to thrash the entire stock mkt\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 0.0529,  0.2104,  0.5702,  0.1432,  0.0234]]), 'timestamp': '2018-11-01T00:35:11Z'}\n",
      "-----------------\n",
      "\n",
      "@w97 ssshhhhh.  Bulls never acknowledge the possibility they might be wrong.  The $MU cult is quite disturbing\n",
      "{'symbol': '$MU', 'score': tensor([[ 0.0083,  0.4694,  0.2244,  0.1878,  0.1102]]), 'timestamp': '2018-11-01T00:35:19Z'}\n",
      "-----------------\n",
      "\n",
      "$CSX max pain is 67.5 for expiry 2018-11-02 Source: http://sweep.ly/maxpain.html\n",
      "$CTAS max pain is 190 for expiry 2018-11-16 Source: http://sweep.ly/maxpain.html\n",
      "$SIG Bearish earnings reversal should drop to it&#39;s 5 EMA  two points lower.\n",
      "Big Trade Blocks for $ANTM (Largest Trade: 0.59% of Volume) https://icebergbot.com?s=ANTM\n",
      "Today&#39;s insight on $NE https://app.seasonaledge.com/tools/I/NE\n",
      "$DWDP I had thought this co could out compete rest of the world by making high qual things from plastic made by cheap natural  gas ?\n",
      "$UNH GOOGLE :OPTUM, ATTORNEY MARK CUKER PHARMACY FRAUD PENSYLVANNIA\n",
      "$GS Yuan Will Hit 7 as China Avoids Heavy Intervention, Goldman Says via http://www.hvper.com/?ref=stw\n",
      "$BB another car comes off the line w QNX every .7 sec! QNX growing at 30% YoY QoQ. All players hard code onto their chips! Yum yum!! $NVDA\n",
      "$MCD bulls have 5 reasons on 10-31 to pay more attention https://dividendbot.com?s=MCD\n",
      "$AMZN boom\n",
      "{'symbol': '$AMZN', 'score': tensor([[ 0.0002,  0.0056,  0.0087,  0.1734,  0.8122]]), 'timestamp': '2018-11-01T00:36:09Z'}\n",
      "-----------------\n",
      "\n",
      "$CTL max pain is 20.5 for expiry 2018-11-02 Source: http://sweep.ly/maxpain.html\n",
      "Today&#39;s insight on $K https://app.seasonaledge.com/tools/I/K\n",
      "The industry average ROA is -1.59%. $MSFT outperforms 81% of its industry peers. https://www.chartmill.com/analyze.php?utm_source=stocktwits&amp;utm_medium=FA&amp;utm_content=PROFITABILITY&amp;utm_campaign=social_tracking#/MSFT?r=fa&amp;key=92318c9b-cb66-4932-8e30-5564fa1dd6a6\n",
      "$CLF What a night in New York.  Perfect weather.  Happy Halloween.  Hope my car doesn’t get egged.\n",
      "@shortvolume $WM is a utility like $RSG\n",
      "$GE Shows a possibility of hitting $8.75 at 35 year trend using quarterly candles:\n",
      "$CTSH max pain is 72.5 for expiry 2018-11-02 Source: http://sweep.ly/maxpain.html\n",
      "@GoldenWhale @dt100 Tim Cook just posted it on Twitter. It’s official, $AAPL will buy $mu. IT WILL SKYROCKET TOMORROW !!!!\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 0.0002,  0.0081,  0.0085,  0.0111,  0.9722]]), 'timestamp': '2018-11-01T00:37:20Z'}\n",
      "-----------------\n",
      "\n",
      "$PKI Of course...\n",
      "$WATT I&#39;m thinking diversified  $AMD $TNDM $NIO $JWN and of course wattup for the long\n",
      "Today&#39;s insight on $CLX https://app.seasonaledge.com/tools/I/CLX\n",
      "$AMZN I don’t give a fukc I bought my shares back today and gonna roll on with Amazon \n",
      "Also bought some apple\n",
      "{'symbol': '$AMZN', 'score': tensor([[ 0.0014,  0.0222,  0.0270,  0.7282,  0.2212]]), 'timestamp': '2018-11-01T00:38:23Z'}\n",
      "-----------------\n",
      "\n",
      "$INTC fair price 38 heard it here first\n",
      "$FCX charts are telling me upwards tomorrow.\n",
      "$AMZN even after ours was green today I’m not sure what to think up or down\n",
      "\n",
      "That they have in store for us\n",
      "{'symbol': '$AMZN', 'score': tensor([[ 0.0135,  0.1646,  0.1884,  0.4837,  0.1498]]), 'timestamp': '2018-11-01T00:38:29Z'}\n",
      "-----------------\n",
      "\n",
      "$AAPL estimates distribution - here’s what 42 Estimize analysts are expecting $AAPL to report for Q4 Gross Margins tomorrow 11/01 AMC.\n",
      "http://www.estimize.com/intro/aapl?chart=scatterplotOrHistogram&amp;metric_name=gross_margins&amp;utm_content=AAPL&amp;utm_medium=metric_distribution&amp;utm_source=stocktwits\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 9.8256e-07,  4.2967e-06,  9.9999e-01,  4.4151e-06,  1.2165e-06]]), 'timestamp': '2018-11-01T00:39:11Z'}\n",
      "-----------------\n",
      "\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 9.8256e-07,  4.2967e-06,  9.9999e-01,  4.4151e-06,  1.2165e-06]]), 'timestamp': '2018-11-01T00:39:11Z'}\n",
      "-----------------\n",
      "\n",
      "$AAPL estimates distribution - here’s what 30 Estimize analysts are expecting $AAPL to report for Q4 Mac Sales tomorrow 11/01 AMC.\n",
      "http://www.estimize.com/intro/aapl?chart=scatterplotOrHistogram&amp;metric_name=mac_unit_sales&amp;utm_content=AAPL&amp;utm_medium=metric_distribution&amp;utm_source=stocktwits\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 1.0996e-06,  6.1430e-06,  9.9998e-01,  7.9770e-06,  1.6875e-06]]), 'timestamp': '2018-11-01T00:39:11Z'}\n",
      "-----------------\n",
      "\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 1.0996e-06,  6.1430e-06,  9.9998e-01,  7.9770e-06,  1.6875e-06]]), 'timestamp': '2018-11-01T00:39:11Z'}\n",
      "-----------------\n",
      "\n",
      "Here’s what 6 Estimize analysts believe $QCOM will report for Q4 Gross Margins on 11/07 AMC.\n",
      "http://www.estimize.com/intro/qcom?chart=historical&amp;metric_name=gross_margins&amp;utm_content=QCOM&amp;utm_medium=metric_update&amp;utm_source=stocktwits\n",
      "$AAPL estimates distribution - here’s what 64 Estimize analysts are expecting $AAPL to report for Q4 iPhone Sales tomorrow 11/01 AMC.\n",
      "http://www.estimize.com/intro/aapl?chart=scatterplotOrHistogram&amp;metric_name=iphone_unit_sales&amp;utm_content=AAPL&amp;utm_medium=metric_distribution&amp;utm_source=stocktwits\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 2.7093e-06,  1.1891e-05,  9.9997e-01,  1.1361e-05,  3.3695e-06]]), 'timestamp': '2018-11-01T00:39:11Z'}\n",
      "-----------------\n",
      "\n",
      "{'symbol': '$AAPL', 'score': tensor([[ 2.7093e-06,  1.1891e-05,  9.9997e-01,  1.1361e-05,  3.3695e-06]]), 'timestamp': '2018-11-01T00:39:11Z'}\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "for i in range(15):\n",
    "    pred = next(score_stream)\n",
    "    if pred:\n",
    "        print(pred)\n",
    "        print('-----------------\\n')\n",
    "    else:\n",
    "        print(\"BOOM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. You have successfully built a model for sentiment analysis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
